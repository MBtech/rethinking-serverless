{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paretoset import paretoset\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "import copy\n",
    "from utils import *\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarks = {\n",
    "#             \"transcode\": [\"cats-1\", \"video-1\", \"video-2\", \"video-3\", \"video-4\", \"video-5\", \"video-6\", \"video-7\", \"video-8\", \"video-9\", \"video-10\"], \n",
    "#             \"pigo-faceblur\": [\"faces2\", \"image-1\", \"image-2\", \"image-3\", \"image-4\", \"image-5\", \"image-6\", \"image-7\", \"image-8\", \"image-9\", \"image-10\"], \n",
    "#             \"pigo-face-detector\": [\"faces2\", \"image-1\", \"image-2\", \"image-3\", \"image-4\", \"image-5\", \"image-6\", \"image-7\", \"image-8\", \"image-9\", \"image-10\"], \n",
    "#             \"openfaas-ocr\": [\"doc\", \"image-1\", \"image-2\", \"image-3\", \"image-4\", \"image-5\", \"image-6\", \"image-7\", \"image-8\", \"image-9\", \"image-10\"], \n",
    "#             \"linpack\": [\"2500\", \"1000\", \"5000\", \"7500\"],\n",
    "#             \"s3\": [\"cats-1\", \"video-1\", \"video-2\", \"video-3\", \"video-4\", \"video-5\", \"video-6\", \"video-7\", \"video-8\", \"video-9\", \"video-10\"]\n",
    "#             }\n",
    "\n",
    "benchmarks = {\n",
    "            \"transcode\": [\"cats-1\", \"video-1\", \"video-2\", \"video-3\", \"video-4\", \"video-5\"], \n",
    "            \"pigo-faceblur\": [\"faces2\", \"image-1\", \"image-2\", \"image-3\", \"image-4\", \"image-5\"], \n",
    "            \"pigo-face-detector\": [\"faces2\", \"image-1\", \"image-2\", \"image-3\", \"image-4\", \"image-5\"], \n",
    "            \"openfaas-ocr\": [\"doc\", \"image-1\", \"image-2\", \"image-3\", \"image-4\", \"image-5\"], \n",
    "            \"linpack\": [\"2500\", \"1000\", \"5000\", \"7500\"],\n",
    "            \"s3\": [\"cats-1\", \"video-1\", \"video-2\", \"video-3\", \"video-4\", \"video-5\"]\n",
    "            }\n",
    "\n",
    "benchmark_naming = {\"transcode\": \"transcode\", \"pigo-faceblur\": \"faceblur\", \"pigo-face-detector\": \"facedetect\", \"classifier\": \"classifier\", \"linpack\": \"linpack\", \"s3\": \"s3\", \"openfaas-ocr\": \"ocr\"}\n",
    "\n",
    "# benchmarks ={\"classifier\": \"bridge-8k-1920x1200\"}\n",
    "base_estimator = 'GP'\n",
    "CPU_LIMITS= [ '250m', '500m', '750m', '1000m', '1250m', '1500m', '1750m', '2000m' ]\n",
    "MEMORY_LIMITS=[ '128Mi', '256Mi', '512Mi', '768Mi', '1024Mi', '2048Mi' ]\n",
    "instance_types = ['m5', 'm5a', 'c5', 'c5a', 'c6g', 'm6g']\n",
    "\n",
    "N_TRAILS = 20\n",
    "N_INIT_SAMPLES = 3\n",
    "N_OPT_TRAILS = 10\n",
    "\n",
    "data_dir = 'analysis_data/'\n",
    "\n",
    "calculate_cpu_mem_costs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x, CPU_LIMITS, MEMORY_LIMITS, instance_types, N_INIT_SAMPLES, function_name, data_name, metric='runtime'):\n",
    "    if base_estimator != \"poly\":\n",
    "        from optimizer.boskopt import Models\n",
    "        model = Models()\n",
    "        model.create_model(function=function_name, base_estimator=base_estimator)\n",
    "    else:\n",
    "        from optimizer.polyreg import Models\n",
    "        model = Models()\n",
    "        model.create_model(function=function_name)\n",
    "\n",
    "    runtimes = []\n",
    "    costs = []\n",
    "\n",
    "    minimum = 600.0\n",
    "    best_config = {}\n",
    "\n",
    "    init_samples = get_random_samples(CPU_LIMITS, MEMORY_LIMITS, instance_types, seed=x, n_samples=N_INIT_SAMPLES)\n",
    "    # init_samples = get_init_samples(CPU_LIMITS, MEMORY_LIMITS, instance_types, seed=x, n_samples=N_INIT_SAMPLES, var=True)\n",
    "\n",
    "    default_config = init_samples[0]\n",
    "    config = default_config\n",
    "    # print(config)\n",
    "\n",
    "    if base_estimator != \"poly\":\n",
    "        from optimizer.boskopt import Models\n",
    "        model = Models()\n",
    "        model.create_model(function=function_name, base_estimator=base_estimator, seed=x)\n",
    "    else:\n",
    "        from optimizer.polyreg import Models\n",
    "        model = Models()\n",
    "        model.create_model(function=function_name)\n",
    "\n",
    "    for i in range(0, N_TRAILS):\n",
    "        success = True\n",
    "\n",
    "        result = get_result_from_data(function_name, data_name, config, MEMORY_LIMITS, metric=metric)\n",
    "        runtimes.append(result)\n",
    "\n",
    "        if result < minimum:\n",
    "            minimum = result\n",
    "            best_config = config \n",
    "        # print(result, config)\n",
    "\n",
    "        model.update(config, result, function_name, success=True)\n",
    "        \n",
    "        if i < N_INIT_SAMPLES-1:\n",
    "            config = init_samples[i+1]\n",
    "        else:\n",
    "            config = model.get_next_config(function_name)\n",
    "        \n",
    "    return model, best_config "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance model across data\n",
    "This part of the notebook creates two scenarios:\n",
    "1. One performance model to predict the performance of all input data (trained using the default data)\n",
    "2. One performance model for each input (trained for that input)\n",
    "and compares their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for function_name in benchmarks.keys():\n",
    "    print(function_name)\n",
    "    comparing_performance = pd.DataFrame(columns=['benchmark', 'data name', 'performance', 'config'])\n",
    "\n",
    "\n",
    "    for x in range(0, N_OPT_TRAILS):\n",
    "        data_name = benchmarks[function_name][0]\n",
    "        print(x)\n",
    "        runtimes = []\n",
    "        costs = []\n",
    "\n",
    "        minimum = 600.0\n",
    "        config_base_model = {}\n",
    "\n",
    "        init_samples = get_random_samples(CPU_LIMITS, MEMORY_LIMITS, instance_types, seed=x, n_samples=N_INIT_SAMPLES)\n",
    "        # init_samples = get_init_samples(CPU_LIMITS, MEMORY_LIMITS, instance_types, seed=x, n_samples=N_INIT_SAMPLES, var=True)\n",
    "\n",
    "        default_config = init_samples[0]\n",
    "        config = default_config\n",
    "        # print(config)\n",
    "\n",
    "        if base_estimator != \"poly\":\n",
    "            from optimizer.boskopt import Models\n",
    "            base_model = Models()\n",
    "            base_model.create_model(function=function_name, base_estimator=base_estimator, seed=x)\n",
    "        else:\n",
    "            from optimizer.polyreg import Models\n",
    "            base_model = Models()\n",
    "            base_model.create_model(function=function_name)\n",
    "\n",
    "        for i in range(0, N_TRAILS):\n",
    "            success = True\n",
    "\n",
    "            result = get_result_from_data(function_name, data_name, config, MEMORY_LIMITS)\n",
    "            runtimes.append(result)\n",
    "\n",
    "            if result < minimum:\n",
    "                # print(result, config, data_name)\n",
    "                minimum = result\n",
    "                config_base_model = copy.copy(config) \n",
    "\n",
    "            base_model.update(config, result, function_name, success=True)\n",
    "            \n",
    "            if i < N_INIT_SAMPLES-1:\n",
    "                config = init_samples[i+1]\n",
    "            else:\n",
    "                config = base_model.get_next_config(function_name)\n",
    "\n",
    "        # best_configs = base_model.get_top_across_types(function_name, marker='all')\n",
    "        # print(best_config)\n",
    "        # config = best_configs[0][1]\n",
    "        # config_base_model = {'instance_type': config[0], 'cpu': config[1], 'memory': config[2]}\n",
    "        # print(config_base_model)\n",
    "\n",
    "        for data_name in benchmarks[function_name][1:]:\n",
    "            # print(data_name)\n",
    "            model, best_config = train_model(x, CPU_LIMITS, MEMORY_LIMITS, instance_types, N_INIT_SAMPLES, function_name, data_name)\n",
    "            # best_configs = model.get_top_across_types(function_name, marker='all')\n",
    "            # print(best_configs)\n",
    "            # config = best_configs[0][1]\n",
    "            # config = {'instance_type': config[0], 'cpu': config[1], 'memory': config[2]}\n",
    "            # print(config)\n",
    "\n",
    "            result_base_model = get_result_from_data(function_name, data_name, config_base_model, MEMORY_LIMITS)\n",
    "            result_spec_model = get_result_from_data(function_name, data_name, best_config, MEMORY_LIMITS)\n",
    "            actual_best = get_best_configuration(instance_types, CPU_LIMITS, MEMORY_LIMITS, function_name, data_name, metric='runtime')\n",
    "\n",
    "            # print(result_base_model, result_spec_model, actual_best)\n",
    "\n",
    "            comparing_performance = comparing_performance.append({'benchmark': function_name, 'data name': data_name, 'performance': result_base_model, 'config': 'Single model-based'}, ignore_index=True)\n",
    "            comparing_performance = comparing_performance.append({'benchmark': function_name, 'data name': data_name, 'performance': result_spec_model, 'config': 'Specific model-based'}, ignore_index=True)\n",
    "            comparing_performance = comparing_performance.append({'benchmark': function_name, 'data name': data_name, 'performance': actual_best[0], 'config': 'Best'}, ignore_index=True)\n",
    "\n",
    "    comparing_performance.to_csv(data_dir + function_name +'-data-dependent.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For execution time objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(20, 4), ncols=6, constrained_layout=True)\n",
    "j = 0\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "for function_name in benchmarks.keys():\n",
    "\n",
    "    comparing_performance = pd.read_csv(data_dir + function_name +'-data-dependent.csv')\n",
    "    \n",
    "    if function_name == 'linpack':\n",
    "        comparing_performance['data name'] = comparing_performance['data name'].astype(str)\n",
    "\n",
    "    comparing_performance = comparing_performance[comparing_performance['data name'].isin(benchmarks[function_name])]\n",
    "    comparing_performance.replace({'Single model-based': 'Generic', 'Specific model-based': 'Data-specific', 'Best': 'Ideal'}, inplace=True)\n",
    "\n",
    "    if function_name == 'linpack':\n",
    "        comparing_performance.drop(comparing_performance[comparing_performance['performance']==10000.0].index, inplace=True)\n",
    "        # print(comparing_performance)\n",
    "        # print(comparing_performance[comparing_performance['data name']==7500])\n",
    "        comparing_performance.replace({7500: '$7500^{*}$'}, inplace=True)\n",
    "\n",
    "\n",
    "    # comparing_performance.replace({10000.0: 'bridge'}, inplace=True)\n",
    "    ax = sns.barplot(data=comparing_performance, x='data name', y='performance', hue='config', ax=axs[j])\n",
    "\n",
    "    j += 1\n",
    "    # ax.axhline(minimum_runtime, ls='--')\n",
    "    # # ax.text(0.5,minimum_runtime+10, \"Minimum\")m\n",
    "    # ax.set_xticks(np.arange(0, 22, 2))\n",
    "    if function_name == \"linpack\":\n",
    "        ax.set_ylim((0, 70))\n",
    "    ax.set_title(benchmark_naming[function_name])\n",
    "    ax.set_ylabel(\"Execution Time (s)\")\n",
    "    ax.set_xlabel(\"Input Data\")\n",
    "    # plt.show()\n",
    "    # plt.xticks(rotation=45)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # if j != 5:\n",
    "    ax.legend().set_visible(False)\n",
    "\n",
    "plt.legend(ncol=1, loc='lower left')\n",
    "plt.savefig('plots/opt_across_data.pdf', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For execution cost objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for function_name in benchmarks.keys():\n",
    "    print(function_name)\n",
    "    comparing_performance = pd.DataFrame(columns=['benchmark', 'data name', 'performance', 'config'])\n",
    "\n",
    "\n",
    "    \n",
    "    for x in range(0, N_OPT_TRAILS):\n",
    "        data_name = benchmarks[function_name][0]\n",
    "        runtimes = []\n",
    "        costs = []\n",
    "\n",
    "        minimum = 600.0\n",
    "\n",
    "        init_samples = get_random_samples(CPU_LIMITS, MEMORY_LIMITS, instance_types, seed=x, n_samples=N_INIT_SAMPLES)\n",
    "        # init_samples = get_init_samples(CPU_LIMITS, MEMORY_LIMITS, instance_types, seed=x, n_samples=N_INIT_SAMPLES, var=True)\n",
    "\n",
    "        default_config = init_samples[0]\n",
    "        config = default_config\n",
    "        # print(config)\n",
    "\n",
    "        if base_estimator != \"poly\":\n",
    "            from optimizer.boskopt import Models\n",
    "            base_model = Models()\n",
    "            base_model.create_model(function=function_name, base_estimator=base_estimator, seed=x)\n",
    "        else:\n",
    "            from optimizer.polyreg import Models\n",
    "            base_model = Models()\n",
    "            base_model.create_model(function=function_name)\n",
    "\n",
    "        for i in range(0, N_TRAILS):\n",
    "            success = True\n",
    "\n",
    "            result = get_result_from_data(function_name, data_name, config, MEMORY_LIMITS, metric='cost')\n",
    "            runtimes.append(result)\n",
    "\n",
    "            base_model.update(config, result, function_name, success=True)\n",
    "            \n",
    "            if i < N_INIT_SAMPLES-1:\n",
    "                config = init_samples[i+1]\n",
    "            else:\n",
    "                config = base_model.get_next_config(function_name)\n",
    "\n",
    "    best_configs = base_model.get_top_across_types(function_name, marker='all')\n",
    "    # print(best_configs)\n",
    "    config = best_configs[0][1]\n",
    "    config_base_model = {'instance_type': config[0], 'cpu': config[1], 'memory': config[2]}\n",
    "\n",
    "    for data_name in benchmarks[function_name][1:]:\n",
    "        print(data_name)\n",
    "        model, _ = train_model(N_OPT_TRAILS, CPU_LIMITS, MEMORY_LIMITS, instance_types, N_INIT_SAMPLES, function_name, data_name, metric='cost')\n",
    "        best_configs = model.get_top_across_types(function_name, marker='all')\n",
    "        # print(best_configs)\n",
    "        config = best_configs[0][1]\n",
    "        config = {'instance_type': config[0], 'cpu': config[1], 'memory': config[2]}\n",
    "        print(config)\n",
    "\n",
    "        result_base_model = get_result_from_data(function_name, data_name, config_base_model, MEMORY_LIMITS, metric='cost')\n",
    "        result_spec_model = get_result_from_data(function_name, data_name, config, MEMORY_LIMITS, metric='cost')\n",
    "        actual_best = get_best_configuration(instance_types, CPU_LIMITS, MEMORY_LIMITS, function_name, data_name, metric='cost')\n",
    "\n",
    "        # print(result_base_model, result_spec_model, actual_best)\n",
    "\n",
    "        comparing_performance = comparing_performance.append({'benchmark': function_name, 'data name': data_name, 'performance': result_base_model, 'config': 'Single model-based'}, ignore_index=True)\n",
    "        comparing_performance = comparing_performance.append({'benchmark': function_name, 'data name': data_name, 'performance': result_spec_model, 'config': 'Specific model-based'}, ignore_index=True)\n",
    "        comparing_performance = comparing_performance.append({'benchmark': function_name, 'data name': data_name, 'performance': actual_best[0], 'config': 'Best'}, ignore_index=True)\n",
    "\n",
    "    comparing_performance.to_csv(data_dir + function_name +'-ec-data-dependent.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(20, 4), ncols=6, constrained_layout=True)\n",
    "j = 0\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "for function_name in benchmarks.keys():\n",
    "\n",
    "    comparing_performance = pd.read_csv(data_dir + function_name +'-ec-data-dependent.csv')\n",
    "    \n",
    "    if function_name == 'linpack':\n",
    "        comparing_performance['data name'] = comparing_performance['data name'].astype(str)\n",
    "\n",
    "    comparing_performance = comparing_performance[comparing_performance['data name'].isin(benchmarks[function_name])]\n",
    "    comparing_performance.replace({'Single model-based': 'Generic', 'Specific model-based': 'Data-specific', 'Best': 'Ideal'}, inplace=True)\n",
    "\n",
    "    if function_name == 'linpack':\n",
    "        comparing_performance.drop(comparing_performance[comparing_performance['performance']==10000.0].index, inplace=True)\n",
    "        \n",
    "        # print(comparing_performance[comparing_performance['data name']==7500])\n",
    "        comparing_performance.replace({7500: '$7500^{*}$'}, inplace=True)\n",
    "\n",
    "\n",
    "    # comparing_performance.replace({10000.0: 'bridge'}, inplace=True)\n",
    "    ax = sns.barplot(data=comparing_performance, x='data name', y='performance', hue='config', ax=axs[j])\n",
    "\n",
    "    j += 1\n",
    "    # ax.axhline(minimum_runtime, ls='--')\n",
    "    # # ax.text(0.5,minimum_runtime+10, \"Minimum\")m\n",
    "    # ax.set_xticks(np.arange(0, 22, 2))\n",
    "    # if function_name == \"linpack\":\n",
    "    #     ax.set_ylim((0, 70))\n",
    "    ax.set_title(benchmark_naming[function_name])\n",
    "    ax.set_ylabel(\"Execution Cost ($)\")\n",
    "    ax.set_xlabel(\"Input Data\")\n",
    "    # plt.show()\n",
    "    # plt.xticks(rotation=45)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # if j != 5:\n",
    "    ax.legend().set_visible(False)\n",
    "\n",
    "plt.legend(ncol=1, loc='lower left')\n",
    "    \n",
    "plt.savefig('plots/opt_across_data-ec.pdf', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "name": "python379jvsc74a57bd04cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
